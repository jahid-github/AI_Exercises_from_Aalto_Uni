{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd6a59lkdfmb8YUPYaYFmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahid-github/AI_Exercises_from_Aalto_Uni/blob/main/Artificial_Intelligence_to_play_Tic_Tac_Toei_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computer Assignment C\n",
        "\n",
        "## Teaching an Artificial Intelligence to play Tic-Tac-Toe\n",
        "\n",
        "For developing systems that can make informed decisions, board games provide good benchmarks. Advances in AI methodology (not just the speed of computers!) have brought us to a point where game-playing AIs can beat human players not only in **Chess** but also in **Shogi** and **Go** [1]. However, this is narrow artificial intelligence, methods trained for a specific purpose and that do not directly generalize to other tasks. For example, the example code below will not learn to drive a car or cook dinner however much you show it driving or cooking.\n",
        "\n",
        "\n",
        "## Reinforcement learning for game play\n",
        "\n",
        "In this exercise, you get to train a game-playing AI from scratch for the classic game of Tic-Tac-Toe (also known as Noughts and Crosses) [2]. We consider the simplest version of the game, where only a `3x3` grid is used. The AI will know the game's hard-coded rules, but simply knowing the rules does not make the AI great at playing. One option would be to show the AI a lot of human-played games as *training data*, but instead, we employ a method called *reinforcement learning* (RL). We will put the RL agent to play games against itself such that it learns how to play the game efficiently.\n",
        "\n",
        "We will consider Q-learning, a model-free reinforcement learning (RL) algorithm. The goal of Q-learning is to learn a policy (what to do), which tells the game-playing agent what action to take under what circumstances (depending on where the opponent puts their mark). It can learn purely by self-play once it has been given the rules of the game. The agent will receive a reward of `+1` if it wins and `-1` if it loses a game. We use a modified implementation following [3], and all code needed for this task is presented below.\n",
        "\n",
        "This time, no data is loaded, and you also need to start by training the model (remember that last time you skipped training). We use a modified implementation following [3], and all code needed for this task is presented below.\n",
        "\n",
        "#### References\n",
        "\n",
        "[1] Steven Strogatz (2018). One Giant Step for a Chess-Playing Machine. [New York Times](https://www.nytimes.com/2018/12/26/science/chess-artificial-intelligence.html)\n",
        "\n",
        "[2] Tic-tac-toe. In Wikipedia, The Free Encyclopedia. URL: https://en.wikipedia.org/wiki/Tic-tac-toe\n",
        "\n",
        "[3] Neil Slater (2017). Game Playing Script(s). URL: https://github.com/neilslater/game_playing_scripts"
      ],
      "metadata": {
        "id": "gIIp9deiGwe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1. Setup methods\n",
        "Run the code below to set up our Q-learning framework for Tic-Tac-Toe.\n",
        "### Utility Code"
      ],
      "metadata": {
        "id": "NAQwfiUbZn1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "   Copyright 2017 Neil Slater\n",
        "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "   you may not use this file except in compliance with the License.\n",
        "   You may obtain a copy of the License at\n",
        "       http://www.apache.org/licenses/LICENSE-2.0\n",
        "   Unless required by applicable law or agreed to in writing, software\n",
        "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "   See the License for the specific language governing permissions and\n",
        "   limitations under the License.\n",
        "\n",
        "   The original code as been modified for educational purposes.\n",
        "'''\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class TicTacToeGame():\n",
        "    def __init__(self):\n",
        "        self.state = '         '  #a string of length 9 that encode the state of the 3*3 board\n",
        "        self.player = 'X'\n",
        "        self.winner = None\n",
        "\n",
        "    def allowed_moves(self):      #find the empty position in the state string\n",
        "        states = []               #store all possible next states\n",
        "        for i in range(len(self.state)):\n",
        "            if self.state[i] == ' ':\n",
        "                states.append(self.state[:i] + self.player + self.state[i+1:])\n",
        "        return states\n",
        "\n",
        "    def make_move(self, next_state):\n",
        "        if self.winner:\n",
        "            raise(Exception(\"Game already completed, cannot make another move!\"))\n",
        "        if not self.__valid_move(next_state):\n",
        "            raise(Exception(\"Cannot make move {} to {} for player {}\".format(\n",
        "                    self.state, next_state, self.player)))\n",
        "\n",
        "        self.state = next_state\n",
        "        self.winner = self.predict_winner(self.state)\n",
        "        if self.winner:\n",
        "            self.player = None\n",
        "        elif self.player == 'X':\n",
        "            self.player = 'O'\n",
        "        else:\n",
        "            self.player = 'X'\n",
        "\n",
        "    def playable(self):\n",
        "        return ( (not self.winner) and any(self.allowed_moves()) )\n",
        "\n",
        "    def predict_winner(self, state):\n",
        "        lines = [(0,1,2), (3,4,5), (6,7,8), (0,3,6), (1,4,7), (2,5,8), (0,4,8), (2,4,6)]  # all possible lines\n",
        "        winner = None\n",
        "        for line in lines:\n",
        "            line_state = state[line[0]] + state[line[1]] + state[line[2]]\n",
        "            if line_state == 'XXX':\n",
        "                winner = 'X'\n",
        "            elif line_state == 'OOO':\n",
        "                winner = 'O'\n",
        "        return winner\n",
        "\n",
        "    def __valid_move(self, next_state):\n",
        "        allowed_moves = self.allowed_moves()  #get all possible next states\n",
        "        if any(state == next_state for state in allowed_moves): #check if the input next_state is in\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def print_board(self):\n",
        "        s = self.state\n",
        "        print('     {} | {} | {} '.format(s[0],s[1],s[2]))\n",
        "        print('    -----------')\n",
        "        print('     {} | {} | {} '.format(s[3],s[4],s[5]))\n",
        "        print('    -----------')\n",
        "        print('     {} | {} | {} '.format(s[6],s[7],s[8]))\n",
        "\n",
        "\n",
        "class Agent():\n",
        "    def __init__(self, game_class, epsilon=0.1, alpha=0.5, value_player='X'):\n",
        "        self.V = dict()  #dictionary to store value\n",
        "        self.NewGame = game_class\n",
        "        self.epsilon = epsilon\n",
        "        self.alpha = alpha\n",
        "        self.value_player = value_player\n",
        "\n",
        "    def state_value(self, game_state):\n",
        "        return self.V.get(game_state, 0.0)\n",
        "\n",
        "    def learn_game(self, num_episodes=1000):\n",
        "        for episode in range(num_episodes):\n",
        "            self.learn_from_episode()\n",
        "\n",
        "    def learn_from_episode(self):\n",
        "        game = self.NewGame()\n",
        "        _, move = self.learn_select_move(game)\n",
        "        while move:\n",
        "            move = self.learn_from_move(game, move)\n",
        "\n",
        "\n",
        "\n",
        "    def learn_from_move(self, game, move):\n",
        "        game.make_move(move)\n",
        "        r = self.__reward(game)\n",
        "        td_target = r\n",
        "        next_state_value = 0.0\n",
        "        selected_next_move = None\n",
        "        if game.playable():\n",
        "            best_next_move, selected_next_move = self.learn_select_move(game)\n",
        "            next_state_value = self.state_value(best_next_move)\n",
        "        current_state_value = self.state_value(move)\n",
        "        td_target = r + next_state_value\n",
        "        self.V[move] = current_state_value + self.alpha * (td_target - current_state_value)  #update value\n",
        "        return selected_next_move\n",
        "\n",
        "    def learn_select_move(self, game):\n",
        "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
        "        if game.player == self.value_player:\n",
        "            best_move = self.__argmax_V(allowed_state_values)\n",
        "        else:\n",
        "            best_move = self.__argmin_V(allowed_state_values)\n",
        "\n",
        "        selected_move = best_move\n",
        "        if random.random() < self.epsilon:  #epsilon-greedy strategy\n",
        "            selected_move = self.__random_V(allowed_state_values)\n",
        "\n",
        "        return (best_move, selected_move)\n",
        "\n",
        "    def play_select_move(self, game):\n",
        "        allowed_state_values = self.__state_values( game.allowed_moves() )\n",
        "        if game.player == self.value_player:\n",
        "            return self.__argmax_V(allowed_state_values)\n",
        "        else:\n",
        "            #return self.__argmin_V(allowed_state_values)\n",
        "            return self.__random_V(allowed_state_values)\n",
        "\n",
        "    def demo_game(self, verbose=False):\n",
        "        game = self.NewGame()\n",
        "        t = 0\n",
        "        while game.playable():\n",
        "            if verbose:\n",
        "                print(\" \\nTurn {}\\n\".format(t))\n",
        "                game.print_board()\n",
        "            move = self.play_select_move(game)\n",
        "            game.make_move(move)\n",
        "            t += 1\n",
        "        if verbose:\n",
        "            print(\" \\nTurn {}\\n\".format(t))\n",
        "            game.print_board()\n",
        "        if game.winner:\n",
        "            if verbose:\n",
        "                print(\"\\n{} is the winner!\".format(game.winner))\n",
        "            return game.winner\n",
        "        else:\n",
        "            if verbose:\n",
        "                print(\"\\nIt's a draw!\")\n",
        "            return '-'\n",
        "\n",
        "    def interactive_game(self, agent_player='X'):\n",
        "        game = self.NewGame()\n",
        "        t = 0\n",
        "        while game.playable():\n",
        "            print(\" \\nTurn {}\\n\".format(t))\n",
        "            game.print_board()\n",
        "            if game.player == agent_player:\n",
        "                move = self.play_select_move(game)\n",
        "                game.make_move(move)\n",
        "            else:\n",
        "                move = self.__request_human_move(game)\n",
        "                game.make_move(move)\n",
        "            t += 1\n",
        "\n",
        "        print(\" \\nTurn {}\\n\".format(t))\n",
        "        game.print_board()\n",
        "\n",
        "        if game.winner:\n",
        "            print(\"\\n{} is the winner!\".format(game.winner))\n",
        "            return game.winner\n",
        "        print(\"\\nIt's a draw!\")\n",
        "        return '-'\n",
        "\n",
        "    def round_V(self):\n",
        "        # After training, this makes action selection random from equally-good choices\n",
        "        for k in self.V.keys():\n",
        "            self.V[k] = round(self.V[k],1)\n",
        "\n",
        "\n",
        "    def __state_values(self, game_states):\n",
        "        return dict((state, self.state_value(state)) for state in game_states)\n",
        "\n",
        "    def __argmax_V(self, state_values):\n",
        "        max_V = max(state_values.values())\n",
        "        chosen_state = random.choice([state for state, v in state_values.items() if v == max_V])\n",
        "        return chosen_state\n",
        "\n",
        "    def __argmin_V(self, state_values):\n",
        "        min_V = min(state_values.values())\n",
        "        chosen_state = random.choice([state for state, v in state_values.items() if v == min_V])\n",
        "        return chosen_state\n",
        "\n",
        "    def __random_V(self, state_values):\n",
        "        return random.choice(list(state_values.keys()))\n",
        "\n",
        "    def __reward(self, game):\n",
        "        if game.winner == self.value_player:\n",
        "            return 1.0\n",
        "        elif game.winner:\n",
        "            return -1.0\n",
        "        else:\n",
        "            return 0.0\n",
        "\n",
        "    def __request_human_move(self, game):\n",
        "        allowed_moves = [i+1 for i in range(9) if game.state[i] == ' ']\n",
        "        human_move = None\n",
        "        while not human_move:\n",
        "            idx = int(input('Choose move for {}, from {} : '.format(game.player, allowed_moves)))\n",
        "            if any([i==idx for i in allowed_moves]):\n",
        "                human_move = game.state[:idx-1] + game.player + game.state[idx:]\n",
        "        return human_move\n",
        "\n",
        "def demo_game_stats(agent):\n",
        "    results = [agent.demo_game() for i in range(10000)]\n",
        "    game_stats = {k: results.count(k)/100 for k in ['X', 'O', '-']}\n",
        "    print(\"    percentage results: {}\".format(game_stats))\n",
        "\n"
      ],
      "metadata": {
        "id": "8Bg5gLAFZowj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2. Make an agent\n",
        "Now it is time to create your game-playing AI agent. Here you choose the name for it (O/X, note that X always starts) and set the learning-rate parameter `alpha` and discount factor `epsilon` for discounting future rewards. We also print out some test stats of the efficiency of this AI before learning (NB: '`-`' stands for a draw)."
      ],
      "metadata": {
        "id": "PQtZw3o2Zut3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create you agent (named 'X') and assign the exploration rate and learning rate\n",
        "agent = Agent(TicTacToeGame, epsilon = 0.1, alpha = 0.5, value_player='X')\n",
        "\n",
        "print(\"Before learning:\")\n",
        "demo_game_stats(agent)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyHxwT5bZ00P",
        "outputId": "65489d25-d280-4d3f-d2d7-9ad765825dbc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before learning:\n",
            "    percentage results: {'X': 58.16, 'O': 29.17, '-': 12.67}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3. Play against your agent\n",
        "Try out how well your agent works. You can play interactive games against the agent by running the cell below. The AI will ask you for your next move and then proceed to make its move. The moves are given by indicating which cell you want to put your mark in:\n",
        "```\n",
        "     1 | 2 | 3\n",
        "    -----------\n",
        "     4 | 5 | 6\n",
        "    -----------\n",
        "     7 | 8 | 9\n",
        " ```\n",
        " Play a couple of games (you should be able to win quite easily...)"
      ],
      "metadata": {
        "id": "shGTe1qdZ56A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.interactive_game()"
      ],
      "metadata": {
        "id": "IWezS07UaBR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4. Train the agent through self-play\n",
        "You can have the agent play against itself for a number of games to learn how to play the game. In the cell below, you will make the agent play 1000 games. After that, try playing against it again. Can you see any difference?"
      ],
      "metadata": {
        "id": "Fe-i4qxWaERR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent.learn_game(num_episodes=1000)\n",
        "print(\"After 1000 learning games:\")\n",
        "demo_game_stats(agent)"
      ],
      "metadata": {
        "id": "l20L8mZvaGbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.interactive_game()"
      ],
      "metadata": {
        "id": "Jfw8LjKCaKGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5. More training and playing\n",
        "Continue training the method:\n",
        "* First for 4000 more games, then try playing again.\n",
        "* Finally, keep training the model for 10000 games more and try playing again.\n",
        "\n",
        "What do you notice? Is the agent getting harder to beat?"
      ],
      "metadata": {
        "id": "upeDfGljaOB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 15000\n",
        "agent.learn_game(num_episodes)\n",
        "print(f\"After {num_episodes} learning games:\")\n",
        "demo_game_stats(agent)"
      ],
      "metadata": {
        "id": "ojJlmKjqbPuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.interactive_game()"
      ],
      "metadata": {
        "id": "z7dzOiAJbUJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## agent.interactive_game()"
      ],
      "metadata": {
        "id": "K-VG2FhqdERC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optional: Tweak the model\n",
        "Now you can tweak the model, and experiment further:\n",
        "* Start by changing the AI identity to \"O\" in `agent.interactive_game('O')` (so that you get to start the games). Re-train and re-try playing.\n",
        "* You can also experiment with changing the learning rate `alpha` and discount factor `epsilon` when creating the agent."
      ],
      "metadata": {
        "id": "5PfRGovUdKnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 'O' into function below to let you start the game\n",
        "agent.interactive_game('O')"
      ],
      "metadata": {
        "id": "2g4ojcgrdQoT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}